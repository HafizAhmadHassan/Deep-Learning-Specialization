{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Last Time\n- Titanic dataset\n- binary split\n- saw Tabular dataset\n- Preprocessing\n- Std deviaton good binary split\n-  automated version of good binary split\n-  applied log to optimise the range of values\n  ","metadata":{}},{"cell_type":"markdown","source":"## Lets Start\n- Lets split male and females to other two groups\n- we need to repeat the same process\n- remove sex\n- split dataset into males frmales\n- run the same piece of code with only male to predict which male kind of survived the titatnic\n- we do same for females","metadata":{}},{"cell_type":"code","source":"cols.remove('Sex')\nismale = trn_df.Sex==1\nmales,females = trn_df[ismale],trn[~ismale]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The bigger predictor for male we can see here is age","metadata":{}},{"cell_type":"code","source":"{o:min_col(males,o) for o in cols}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The bigger preddicter of females are whether they are in first class or not you can check PClass parameter","metadata":{}},{"cell_type":"code","source":"{o:min_col(females,o) for o in cols}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Decision\n - It is series of binary split where at very deep level on leaf node we have much more confident and stronger prediction about target value in our case it is survival\n - Males and kids order than age 6\n - Female with Class 1st we have 4 groups\n - we can do again for 8 groups\n\n## Library\n- Scikit Learn classical nondeep learning\n- max leaf node. we have\n- split on sex in mnumeric is bit wiered in picture we can see in binary form\n- here we can seee females further split where in female leaf we can see 116 are survived and 4 did not [4,116]\n- On the other hand in males [350,68] 350 died\n- We like it becuase exploratory dataset analysis\n- Gini is another way of measuring how good split is. code is written\n\n## Question\n\n\nHow likely it is you go to that sample and grab one item going again and grab another item how likely it is you gonna grab the same item each time?\n- If the entrire leaf node just people survived or just people not survived the probability would be 1\n- if it was not exactly equal it could be 0.5 because half of them oved to another split and half of them move to another one 1/number of splits = 1/2 i would not grab the same item. if ended up on one node I would definitely end up grabbing the same split would be 1 . so probability is 1 because 1/1 = 1\n- In our grapgh we have gini 0.5 in leaf [55,54] pretty much 50 50.. Gini = 0.006 [4,116] because most of data is mostly on 1 class.\n- This decision tree we calculate absolute mean eror previously we have 0.336\n\n- we got 0.22 not improved much Small dataset 1h is best.\n   ","metadata":{}},{"cell_type":"code","source":"def gini(cond):\n    act= df.loc[cond,dep]\n    return 1 - act.mean()**2 (1-act).mean()**2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gini(df.Sex=='female'),gini(df.Sex=='male')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_absolute_error(avl_y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Bit Further\n- we give minimum sample per leaf node 50\n-  you can see in the leaf node we have 2nd node from left to right we have 67 sample.\n-  keep splitting we have this tree unless minim sample we have 50\n-  for males it grows alot\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Kaggle Submission\n\n-  Kaggle competition we submit it to leaderboard\n-  Advice : dont wait for months to submit to ladder board everyday regardless of waiting for months\n-  we applied same preprocessing steps\n-  it is simple not much preprocessing steps required .. no needd of dummy variables for class u can split 1 2 or 3.\n-  Decision tree care about ordering of data\n-  no care about long tail distribution etc outlier\n-  LogFare we did for graph  look better.\n## Recommendation \n- Tabular data we start with Decision tree\n\n## Embarked column\n- first latter of city pandas convert to category\n- we can check codes\n- strings to numbers we did for visual purpose\n- if you C in one group and QS into other you can decide based on code for example C=0 Q=1 S=2 I can say if value is greater than 0 another is rest\n- somethings more messing variable\n#### dummy variables we us because we can have levels sense in Visualisation\n- if I have less than 4 levels I use dummy variables and numeric for greater than equal 4.\n\n## Question\n- Can I grow the tree further.\n- I mean we could but we have 50 samples its not eally leaf does not have data for useful prediction there are limitations\n\n  \n## Random Forest\n- Bagging idea its got error on prediciton\n- I could build another decision tree in different way some errors high some low\n- end up building 100 decision trees\n- assuming they are not biased.\n- Errors are not coorelated\n- Average of errors will be 0\n- average of prediction is coect prediction\n\n### idea \nIf we can build uncorelated and unbiased models we can average them we can get something better than individual \n\n### How unbiased uncorelated\n- with different subset of data\n- take random half of data build decision tree\n- another random half of data build decision tree\n- each of them not good but will be unbiased\n- better than nothing\n- wont be coorelated because random choosen\n\n## lets Code\n\n- 0.75 mean 75% of data each time n is number of samples\n- n*prop\n- get treee stick 100 range\n- for each tree get prediction\n- stack average of prediction\n- mean_absolute error","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Another variation\n- randomly select diferent subset of columns\n- each time they take different\n- as random as possible\n\n## Clasifier\n- Scitkit 100 trees\n- 0.188\n\n ### Submit to Kaggle\n \n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Favourite thing : Feature importance\n- we look undeline decision tree\n- we can look what columns we find to split on\n- improve sex Gini 0.47 to PClass in visulisation 0.38 we got (0.47-0.38)= 0.14. we can track Gini improvements\n- We do that for every decision tree\n- we Add them together per Column\n- Feature importance plot we got\n- It tells you how important features are\n- We can see in graph Sex is 1st most mportant and PClass second most\n## Helpful Case Study\n- As random forest dont care about distribution categorical and numeric\n- Random Forest helpful to find out important features for any tabular\n- If you have big dataset hundereds of column. First do this to get most important one\n- Credit Scoring Target: i was given 700 columns and I went to 30 Most important. I went to marketing team tell them this one impotant consider marketing team paid million of dollar filled project by end come up with these\n\n## Other things you can do.\n- Chapter 8 of the book\n- Auction prices ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handwavy Theory More Tree lower error always happen\n\n- Chapter 8 shows\n- estimators tree ccuracy improve\n- I added upto 40 more anad more tree\n\nEnd up production Random Forest?\nIts extremely fast performace\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Out of Bag\n\n- Remember Each tree 25% we did not train on.\n- in some situation ou train all and endup not having validation data.\n- The reason because for For each tree 25% rows to validate. we can average accuracy on all the tree which they are not part of tree OOB\n- Out of bag prediction.\n- Bagging is meta method is approach to ensemble\n- \n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Advice and Why We are Fan\n- I would personally start with Random Forest First\n- Reason\n1. Hard to Mess up Preprocessing\n2. Quick Insights\n3. Confident in prediction in particular row. For example loan payment we are interested in how likely he will pay...but how confident we are.\n4. Lets say you rejected the loan you can say reason\n5. Feature importance\n6. Rudundance highly co relation we can find\n7. how prediction vary if you dont use specific crecteistics \n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Prediction confidence Mean Vs Std\n- Stack prediction take a mean\n- What if we take standard deviation\n- It means all tree predict something different we dont know what we doing\n-  ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Removing Low-Importane Variable\n- cleaning up data is helpful so we remove","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Partial Dependence\n- Relation between dep_var target value and column\n- For any machine learning model we can use\n- Bulldozer data Year Mode and price it is sold for graph\n- you might think you can make it by averaging each year price but actually does not done\n- consider biggest factor is airconditioning makes more expensive buy\n- so in the late years the air conditioner was not present but now we have so there is coorelation going on\n- impact of yearmade and AC\n## So Process\n1. we take every single row for the year it was made -- for yearMade column set it to 1950\n2. For every row we will make prediction if it was made in 1950\n3. we repeat it for 1951,1952 ...2000\n4. You can do more than one column for example two way partial dependence","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## How decide why particular prediction was Made?\n- all we do Tree intrepreter customer coming ask for loan\n- Tree1 what was path for example RF look income , zipcode change in Gini we will check that will tell u feature iimportance\n- we can see plot for auction price prediction\n\n## Auction price prediction\n1. shows feature importance Red Prediction go down\n2. Green prediction goes up\n\n### Delete Bad OOB Tree?\n- No we will not because otherwise it will be baised.\n- Esembled of bagged models\n### Structuring Ensemble\n- For example i do 100 tree 10 i take out do average, 20 i take do average .. rest do average does it make impact.. for sure no because average of average still average\n- Some Ensemble we can structure\n\n### SHAP and LIME Model Explanability\n- I would lean towards more towards Random forest feature importance\n- its data dependent\n- Briemen Focused on its Statistical modeling\n\n  ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## OverFitting\n- basically no adding more tree make accuracy\n- if you dont have enough trees and let the tree very deep then may be u can have oerfitting\n- adding lots of randomly generated columns giving misleading dataset and confuse RF - its not useful\n- ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Gradient Boosting\n- Normalisation u dont have to worry about\n- Go : Expalin.ai gradient Boosting\n- Rather fitting same size tree . we fit very very small different size tree\n- Residual Predicted-actual\n- small trees we have predicting residual \n- you take sum of trees instead sum then we call it boosting instead og bagging - ensemble technique\n- When it applied called Gardiant boosting Machine\n- Its breakable its not first choice\n- GBM is more accurate in general\n- you can watch our podcast\n","metadata":{}},{"cell_type":"markdown","source":"## Go to Kaggle\n- How random forests really work\n- Paddy disease competition we picked\n- Jermy was number 1\n- walkthrough is available 08-first-step check github\n- LocL MAchine we have fastkaggle","metadata":{}},{"cell_type":"markdown","source":"## FastKaggle\n- run setup_comp\n- if you on kaggle it will return path otherwise it return path\n- we have wawlkthrough likes checkn it out\n- you cant hide truth in kaggle\n- is it overfit ? or actually right\n- we work in subset on dataset in kaggle\n- structuring code and analysis so u can keep improving in 3 months\n- Focus : good validation set, how quickly try iteration rapidly u can experiment, try 80 things not openai like model. Dont go force facy model .. go so u can easily iteratioe\n- ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Notebook\n- random seed because sharing notebook everyone will have same results\n- path.ls we can see data","metadata":{}},{"cell_type":"markdown","source":"## Looking data\n- Grab image PIL pythom image libray in tensors we say row and column\n- PIL column x row\n- Decode jpeg is slow\n\n\n## fastcore.parallel\n- fastcore.parallel will does python ai library function run in Parallel\n\n- 10403 have size 408 x 640\n\n- 4 images have differnt we need to preprocessing\n\n- we wont do initial resizing\n\n- ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Image Dataloafets\n-  method squish 480x480 as oppose cropping random\n-  item tranform Mini batch and data augmentation\n-  mini batch pass throuugh pass theough augmentation it will grab random subset 238x128 pixel\n-  show_batch works for anything fastaudio fastvision\n-  ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## I made model so I could understand data Light model\n- that can train quickly\n## Check : NOtebook Kaggle Best Vision model \n- Look which one can be fine tuned easily\n- we checked tranformer on pet dataset\n- Check the table of results\n- I pick which resnet26d accuracy is 0.06 and can be fastly trained\n- we dont first look layers parameters bla bla bla we should do input output check\n  ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Futher Submit\n- move ahead lr_find\n- i go futher up 0.01\n- 3 epochs rapidly\n- read CSV file and submit","metadata":{}},{"cell_type":"markdown","source":"## Now Dataloader for test set\n- test_dl has no labels its key difference between train dataloaders\n- ","metadata":{}},{"cell_type":"markdown","source":"## Learner and prediction\n- Get prediction\n- ask decoded = True - dont return me probability give me index - label\n- dls.vocab to see names\n- Look dictionary and pandas to see index with string together","metadata":{}},{"cell_type":"markdown","source":"## replace column label and kaggle submission\n- we replace column again and suubmit to kaggle\n- iterating rapidly everything should be easy .. kaggle submitting should be fast\n- api.competintion submit (its fast process)\n- Conclution : top 80% is something we can start with\n- then tomorrow u can try slighly better one","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Automated sharing Notebook\n1. Kaggle : First Step Road to Top\n2. Why Public notebook : same brutality of feedback entring feedback. wether you can communicate people find interesting\n3. Ask community : What you think i should improve? how to make better\n4. Create model is better and communicate well you are good data scientist\n\n## Question\n- Different models and different notebook we try?\n- Check directory course/paddy --> first-step.ipynb\n- duplicated 09-small model\n- give proper name\n- its very low tech approach find it working really well\n- link to git commit\n- one submission we have then we move on creating another one\n- filesystems and filenames are good\n## Question\n- AutoML : used less than anybody i know\n- hyperparameter optimisation never\n- like to think more like scientist\n- hypothetial analysis\n- For example In Best vision model i dont go to check each model and do grid testing\n- instead step one to find which thing is metter\n- e.g squish or crop?\n- not all architetures but one orr two versions of possible families\n- took 20 minutes\n- answer no : so every single case for example squish is better\n- we dont need to do grid search\n## Another Classic Example\n- learning rates grid search with learning ates\n- people train thousand models with different learning rates\n- Liesle smith learning rate finder is the best\n- I mean Neural network vs GBM they have special places\n- Computer vision i will go deep learning models\n## My Search\n- In best vision model notebook I will go for two tables\n1. Pets dataset : which one is best to pre-trained model\n2. Planet dataset same we will check\n- Convnext is right up in both cases\n- RF is fastest GBM slighly better\n- GBM i would run Hyperparameter sweep\n\n## My Computer vs Kaggle\n- per epoch in my computer 1 min on Kaggle it is 4 min\n- Kaggle gpu not amazing but not bad\n- they have two virtual gpu\n- 8 physical cpu per gpu\n\n## Time Spening ?\n- Data reading 680 x 480\n- we have 128 pixels size\n- step1 : kaggle itereation faster\n- step2 : resize\n- call function resize images max_size 256 recuse=true\n\n## Results\n- went down 4 times faster with no loss of accuracy\n- We ccan cpu and gpu usage\n## I want to use better model\n- graph speed vs accuracy on family of models we can check best vision notebook\n- convext tiny 122k we choose\n- so we start with small resnet then convext check notebook small Roads to Top Part 2\n- keep on seeing upcoming architecture\n- convext_small_122K options : small ,tiny larger see in graph\n- kaggle it takes 45 sec my computer 20 sec\n## we can use crop instead of squish\n- I call train function by passing arguements\n- we check error rates same\n## padding\n- does not distort o loose information\n- squish distort\n- crop we loose\n- downside side pixel contains zeros\n- we see no huge differences well\n\n## what we will do test time data image augmentation?\n- We do same pic  different version , slightly rotated, flipped horizontally, slightly rotated, wrapped\n- may be for model some of them better than other\n- little mini bagging\n- passed to model check average prediction\n- called lean.tta\n- In jermy computer it was better\n\n## About Images\n- They dont need to square\n- no need to be same size\n- given all images are 640 x480 we can pick aspect ratio for example 256 x 192\n- resize with same aspect ratio. rectangular\n- 12 epochs\n- now 0.22\n- tta we can see 0.0197 we are under 2 percent\n- very mechanistic approach\n- we can use computer vision dataset\n\n## Submission with Cute idea\n- Take vocab from learner make np.array in our case it is list of 10 things\n- index into that vocab with our indicies\n- vocab is 10 things index is 4000-5000 things but it will give thoousands results by mapping ","metadata":{}},{"cell_type":"code","source":"vocab=np.array(learn.dls.vocab)\nresults = pd.Series(vocab[indxs],name='indxs') ## very fast optimised on CPU \n# when submitted we are top 25 %","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Kaggle submission\n- top 25%\n## Question Augmentation\n- tta during traing always do augmentation during training\n- we are varying each image slightly\n- tta when we call in fast ai use exact same thing in different data we pass\n## Model takes Rectangle input why care for square?\n- Most of times we have wide variety of aspect ratio\n- tall skinny one as wide short one does not make sense to rectangle some of them eally destroy square is best compromise\n\n## Interesting Published Experiment\n- batch things similar aspect ratio togethe\n- kind of median rectangle for those have good results\n- 99 percent people did with square things\n\n## padding issue?\n- 4:3 aspect ratio\n- by default we do reflection padding\n- another copying\n- in the end computer wants to know end of image\n- reflective computer create spikes what these are kind of assuming","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}